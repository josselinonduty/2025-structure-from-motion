{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Structure from Motion (SfM) Notebook\n",
        "\n",
        "This notebook demonstrates how to generate a 3D point cloud from a set of images using Structure from Motion (SfM) techniques.\n",
        "We'll use OpenCV for feature extraction, matching, and triangulation.\n",
        "Ensure you have the necessary images in the specified directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Set paths\n",
        "data_in = \"data\"\n",
        "data_set = \"globe\"\n",
        "data_set_ext = \"JPG\"\n",
        "data_out = \"out\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(data_out, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Feature Extraction and Matching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f41c7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_original_image_id(id):\n",
        "    return 31 - id if id < 16 else id - 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load images\n",
        "image_paths = sorted(glob.glob(os.path.join(data_in, data_set, f\"*.{data_set_ext}\")))\n",
        "images = [cv2.imread(img_path, cv2.IMREAD_COLOR_RGB) for img_path in image_paths]\n",
        "# Rearrange in this order (index): from 31 to 16 then 0 to 15\n",
        "images = images[31:15:-1] + images[0:16]\n",
        "\n",
        "print(f\"Number of images: {len(images)}\")\n",
        "\n",
        "\n",
        "# Initialize SIFT detector\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "# Extract features and match them\n",
        "keypoints = []\n",
        "descriptors = []\n",
        "for img in images:\n",
        "    kp, des = sift.detectAndCompute(img, None)\n",
        "    keypoints.append(kp)\n",
        "    descriptors.append(des)\n",
        "\n",
        "# Match features between images\n",
        "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "matches = {}\n",
        "for i in range(len(images) - 1):\n",
        "    matches[(i, i + 1)] = bf.match(descriptors[i], descriptors[i + 1])\n",
        "    print(\n",
        "        f\"Number of matches between images {get_original_image_id(i)} and {get_original_image_id(i + 1)}: {len(matches[(i, i + 1)])}\"\n",
        "    )\n",
        "\n",
        "# Sort matches by distance\n",
        "matches = {k: sorted(v, key=lambda x: x.distance) for k, v in matches.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23618eec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot matches\n",
        "for i in range(3):\n",
        "    img_i = cv2.drawMatches(\n",
        "        images[i],\n",
        "        keypoints[i],\n",
        "        images[i + 1],\n",
        "        keypoints[i + 1],\n",
        "        matches[(i, i + 1)],\n",
        "        None,\n",
        "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
        "    )\n",
        "\n",
        "    plt.imshow(img_i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec523a9",
      "metadata": {},
      "source": [
        "## Step 2: Eliminate Outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15b67d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove outliers from matches using cv2.findHomography\n",
        "\n",
        "for i, j in matches.keys():\n",
        "    kp1 = np.array([keypoints[i][m.queryIdx].pt for m in matches[(i, j)]])\n",
        "    kp2 = np.array([keypoints[j][m.trainIdx].pt for m in matches[(i, j)]])\n",
        "\n",
        "    H, mask = cv2.findHomography(kp1, kp2, cv2.RANSAC, 5.0)\n",
        "\n",
        "    matches[(i, j)] = [m for m, msk in zip(matches[(i, j)], mask) if msk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e2bcdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot matches\n",
        "for i in range(3):\n",
        "    img_i = cv2.drawMatches(\n",
        "        images[i],\n",
        "        keypoints[i],\n",
        "        images[i + 1],\n",
        "        keypoints[i + 1],\n",
        "        matches[(i, i + 1)],\n",
        "        None,\n",
        "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
        "    )\n",
        "\n",
        "    plt.title(\n",
        "        f\"Matches between images {get_original_image_id(i)} and {get_original_image_id(i+1)}\"\n",
        "    )\n",
        "    plt.imshow(img_i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7327ce70",
      "metadata": {},
      "source": [
        "## Step 3: Essential Matrix Estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the best image pair (with the most matches)\n",
        "best_pair_idx = np.argmax([len(m) for m in matches.values()])\n",
        "best_matches = list(matches.items())[best_pair_idx][1]\n",
        "\n",
        "print(f\"Number of matches in best pair: {len(best_matches)}\")\n",
        "best_pair_i, best_pair_j = list(matches.keys())[best_pair_idx]\n",
        "print(\n",
        "    f\"Best pair: ({get_original_image_id(best_pair_i)}, {get_original_image_id(best_pair_j)})\"\n",
        ")\n",
        "\n",
        "# Get points from matches\n",
        "pts1 = np.float32([keypoints[best_pair_i][m.queryIdx].pt for m in best_matches])\n",
        "pts2 = np.float32([keypoints[best_pair_j][m.trainIdx].pt for m in best_matches])\n",
        "\n",
        "# Compute the essential matrix\n",
        "K = np.array(\n",
        "    [\n",
        "        [1698.873755, 0.000000, 971.7497705],\n",
        "        [0.000000, 1698.8796645, 647.7488275],\n",
        "        [0.000000, 0.000000, 1.000000],\n",
        "    ]\n",
        ")\n",
        "E, mask = cv2.findEssentialMat(\n",
        "    pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=5.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db00bf73",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_outliers = np.sum(mask == 0)\n",
        "print(f\"Number of outliers: {num_outliers}\")\n",
        "\n",
        "num_inliers = np.sum(mask == 1)\n",
        "print(f\"Number of inliers: {num_inliers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07746afb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the best pair of images\n",
        "img_best_pair = cv2.drawMatches(\n",
        "    images[best_pair_i],\n",
        "    keypoints[best_pair_i],\n",
        "    images[best_pair_j],\n",
        "    keypoints[best_pair_j],\n",
        "    best_matches,\n",
        "    None,\n",
        "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
        ")\n",
        "\n",
        "plt.title(\n",
        "    f\"Matches between images {get_original_image_id(best_pair_i)} and {get_original_image_id(best_pair_j)}\"\n",
        ")\n",
        "plt.imshow(img_best_pair)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Camera Pose Estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9803ef5",
      "metadata": {},
      "outputs": [],
      "source": [
        "R1, R2, t = cv2.decomposeEssentialMat(E)\n",
        "\n",
        "# Select the correct rotation and translation\n",
        "P1 = np.hstack((np.eye(3), np.zeros((3, 1))))  # First camera at (I|0)\n",
        "P2s = [\n",
        "    np.hstack((R1, t)),\n",
        "    np.hstack((R1, -t)),\n",
        "    np.hstack((R2, t)),\n",
        "    np.hstack((R2, -t)),\n",
        "]\n",
        "\n",
        "\n",
        "def count_positive_depth(P1, P2, pts1, pts2, K):\n",
        "    \"\"\"Count the number of triangulated points with positive depth in both camera frames\"\"\"\n",
        "    points_4d = cv2.triangulatePoints(K @ P1, K @ P2, pts1.T, pts2.T)\n",
        "    points_3d = (\n",
        "        points_4d[:3] / points_4d[3]\n",
        "    )  # Convert to 3D by dividing by homogeneous coord\n",
        "\n",
        "    # Convert to second camera's coordinate system\n",
        "    points_cam2 = P2[:, :3] @ points_3d + P2[:, 3].reshape(3, 1)\n",
        "\n",
        "    # Count points where Z > 0 in both camera frames\n",
        "    return np.sum((points_3d[2] > 0) & (points_cam2[2] > 0))\n",
        "\n",
        "\n",
        "# Select the best P2 based on positive depth count\n",
        "P2 = max(P2s, key=lambda P2_i: count_positive_depth(P1, P2_i, pts1, pts2, K))\n",
        "\n",
        "print(\"R:\", P2[:, :3])\n",
        "print(\"t:\", P2[:, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b001abc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Double check the results\n",
        "_, R_, t_, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
        "print(\"R_:\", R_)\n",
        "print(\"t_:\", t_.reshape(-1))\n",
        "\n",
        "print(f\"{np.allclose(R_, P2[:, :3])=}\")\n",
        "print(f\"{np.allclose(t_.reshape(-1), P2[:, 3])=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bad0ba67",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstruct 3D points using cv2.triangulatePoints\n",
        "points = cv2.triangulatePoints(K @ P1, K @ P2, pts1.T, pts2.T)\n",
        "points /= points[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d293d4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import open3d as o3d\n",
        "\n",
        "cloud = o3d.geometry.PointCloud()\n",
        "cloud.points = o3d.utility.Vector3dVector(points.T[:, :3])\n",
        "\n",
        "o3d.io.write_point_cloud(f\"{data_out}/{data_set}-2.ply\", cloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b69f58e",
      "metadata": {},
      "source": [
        "# Step 5: Growing step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec77b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# processed = set([best_pair_i, best_pair_j])\n",
        "# k_break = 0\n",
        "\n",
        "# points_cum = points.copy()\n",
        "\n",
        "# while len(processed) < len(images) and k_break < len(images):\n",
        "#     print(f\"Step {k_break+1}/{len(images)}\")\n",
        "\n",
        "#     for grow_id, grw_image in enumerate(images):\n",
        "#         if grow_id in processed:\n",
        "#             continue\n",
        "\n",
        "#     k_break += 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
